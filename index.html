<!DOCTYPE html>
<html>
<head>
<title>Shilin Fu</title>

<style media="screen" type="text/css">
    table {
      counter-reset: rowNumber;
    }

    .numbertable tr::before {
      display: table-cell;
      vertical-align: middle;
      counter-increment: rowNumber;
      content: counter(rowNumber) ".";
      padding-right: 0.3em;
      text-align: left;
    }

body
{
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1080px;
  font-family: Produkt, Times New Roman, Helvetica, sans-serif;
  font-size: 14px;
  background: #ffffff;
}
</style>

</head>

<body>
		
	<table align="center" cellspacing="10">
	<tr>
	<td align="center"><img border=0 height=254 width=200 src="shilinfu.jpg"></td>
	<td align="center">
			<td align="center"><h2>Shilin Fu</h2>
			<p><font size=+1>&#20613;  &#19990; &#26519;</font></p>
			<p><font size=+1>Ph.D Student </font></p>
			<p><font size=+1>Dalian University of Technology </font></p>						
			<p><font size=+1>Email: fslecho at gmail dot com</font></p>
			<p><font size=+1>WeChat: slfecho</font></p>
			</td>			
	</td>		
	</tr>
	</table>
	
	
	<h2>Biography</h2>
	<hr/>
	<p><font size="3">
		
		Shilin Fu is the Ph.D student at the Dalian University of Technology, supervised by Prof. Xiaopeng Wei and Assoc. Prof. Liang Zhang. His research focuses on computational photography, computer vision, and multimodal learning. His work aims to address the unknown degradation in RGB images (e.g., blur, sensor noise,  low dynamic range (LDR),  insufficient  illumination, etc.) by introduce the auxiliary vision modals (e.g., event information), which can facilitate artificial vision systems to achieve all-weather perception and understanding in harsh scenes. 
		<br>
		<br>
		Besides, His early work was on emotion recognition and video action recognition, seeking to understand and analyze affective and action states using RGB images or human skeleton sequence.
		
	</font></p>	

	
	<h2>Selected Publications&nbsp

	
	<!-- <table class="numbertable" cellspacing="10">
	<hr>
	<tbody>
		<tr>
      		<td>
				<font size="3">LuSh-NeRF: Lighting up and Sharpening NeRFs for Low-light Scenes
				<br>
				<i>  Zefan Qu, <b>Ke Xu</b><span lang="EN-US" style="mso-bidi-font-size:8pt;font-family:Wingdings;mso-ascii-font-family:'Times New Roman';mso-hansi-font-family:'Times New Roman';color:black;mso-char-type:symbol;mso-symbol-font-family:Wingdings">*</span>, Gerhard Hancke, Rynson Lau</i>
				<br>
				<b><i><font color="#770043">Proc. NeurIPS</font></i></b>, Dec. 2024
				<br>
				[<a href='https://arxiv.org/pdf/2411.06757'><b>Paper</b></a>|<a href='https://github.com/quzefan/LuSh-NeRF'><b>Code</b></a>]
			</td>
		</tr>

		<tr>
			<td>
				<font size="3">Gaussian Surfel Splatting for Live Human Performance Capture
				<br>
				<i>  Zheng Dong, <b>Ke Xu</b>, Yaoan Gao, Hujun Bao, Weiwei Xu, Rynson Lau</i>
				<br>
				<b><i><font color="#770043">ACM Trans. on Graphics (Proc. ACM SIGGRAPH Asia)</font></i></b>, Dec. 2024
				<br>
				[<a href='https://drive.google.com/file/d/1PGigrWCZ0NkxPmBZ9CgUp6ZWgYe71Dhz/view?usp=sharing'><b>Paper</b></a>|<a href=''><b>Code</b></a>|<a href='https://www.youtube.com/watch?v=M4fD3GgjZTI'><b>Video1</b></a>|<a href='https://www.youtube.com/watch?v=MGIlBT_JZNk'><b>Video2</b></a>]
			</td>
		</tr>
	</tbody>
	</table> -->
	


</body>

</html>

